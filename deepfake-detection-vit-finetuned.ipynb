{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12391014,"sourceType":"datasetVersion","datasetId":7813517,"isSourceIdPinned":false}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:02.074654Z","iopub.execute_input":"2025-07-12T05:24:02.075277Z","iopub.status.idle":"2025-07-12T05:24:02.080462Z","shell.execute_reply.started":"2025-07-12T05:24:02.075252Z","shell.execute_reply":"2025-07-12T05:24:02.079713Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# pip install timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:02.093777Z","iopub.execute_input":"2025-07-12T05:24:02.093959Z","iopub.status.idle":"2025-07-12T05:24:04.682226Z","shell.execute_reply.started":"2025-07-12T05:24:02.093945Z","shell.execute_reply":"2025-07-12T05:24:04.681542Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms, models\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\nimport sklearn\nimport kagglehub\nimport random\nfrom tqdm import tqdm\nimport timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:04.683236Z","iopub.execute_input":"2025-07-12T05:24:04.683500Z","iopub.status.idle":"2025-07-12T05:24:15.589403Z","shell.execute_reply.started":"2025-07-12T05:24:04.683473Z","shell.execute_reply":"2025-07-12T05:24:15.588858Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"print(f\"OS version: {os.name}\")\nprint(f\"OpenCV version: {cv2.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"TorchVision version: {torchvision.__version__}\")\nimport matplotlib\nprint(f\"Matplotlib version: {matplotlib.__version__}\")\nprint(f\"Scikit-Learn version: {sklearn.__version__}\")\nprint(f\"Kagglehub version: {kagglehub.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:15.590125Z","iopub.execute_input":"2025-07-12T05:24:15.590335Z","iopub.status.idle":"2025-07-12T05:24:15.596200Z","shell.execute_reply.started":"2025-07-12T05:24:15.590318Z","shell.execute_reply":"2025-07-12T05:24:15.595443Z"}},"outputs":[{"name":"stdout","text":"OS version: posix\nOpenCV version: 4.11.0\nNumPy version: 1.26.4\nPandas version: 2.2.3\nPyTorch version: 2.6.0+cu124\nTorchVision version: 0.21.0+cu124\nMatplotlib version: 3.7.2\nScikit-Learn version: 1.2.2\nKagglehub version: 0.3.12\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def label_from_path(path):\n    return \"real\" if \"original\" in path else \"fake\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:15.597818Z","iopub.execute_input":"2025-07-12T05:24:15.598021Z","iopub.status.idle":"2025-07-12T05:24:15.610715Z","shell.execute_reply.started":"2025-07-12T05:24:15.598005Z","shell.execute_reply":"2025-07-12T05:24:15.610125Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:15.611359Z","iopub.execute_input":"2025-07-12T05:24:15.611581Z","iopub.status.idle":"2025-07-12T05:24:15.685123Z","shell.execute_reply.started":"2025-07-12T05:24:15.611531Z","shell.execute_reply":"2025-07-12T05:24:15.684381Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"print(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:15.685901Z","iopub.execute_input":"2025-07-12T05:24:15.686146Z","iopub.status.idle":"2025-07-12T05:24:15.700101Z","shell.execute_reply.started":"2025-07-12T05:24:15.686127Z","shell.execute_reply":"2025-07-12T05:24:15.699464Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:15.700895Z","iopub.execute_input":"2025-07-12T05:24:15.701084Z","iopub.status.idle":"2025-07-12T05:24:15.899670Z","shell.execute_reply.started":"2025-07-12T05:24:15.701060Z","shell.execute_reply":"2025-07-12T05:24:15.898668Z"}},"outputs":[{"name":"stdout","text":"Sat Jul 12 05:24:15 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P0             27W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"torch.manual_seed(42)\nrandom.seed(42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:15.900883Z","iopub.execute_input":"2025-07-12T05:24:15.901663Z","iopub.status.idle":"2025-07-12T05:24:15.910478Z","shell.execute_reply.started":"2025-07-12T05:24:15.901628Z","shell.execute_reply":"2025-07-12T05:24:15.909876Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"noobcoder27/processed-faces\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:15.911138Z","iopub.execute_input":"2025-07-12T05:24:15.911331Z","iopub.status.idle":"2025-07-12T05:24:16.026211Z","shell.execute_reply.started":"2025-07-12T05:24:15.911316Z","shell.execute_reply":"2025-07-12T05:24:16.025464Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/processed-faces\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"base = '/kaggle/input/processed-faces/processed_faces'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:16.028853Z","iopub.execute_input":"2025-07-12T05:24:16.029492Z","iopub.status.idle":"2025-07-12T05:24:16.032710Z","shell.execute_reply.started":"2025-07-12T05:24:16.029466Z","shell.execute_reply":"2025-07-12T05:24:16.032086Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def load_images_and_labels(split): \n    image_paths, labels = [], [] # image_path -> stores the image path of split dataset and labels[] -> corresponding labels \n    for label in ['real', 'fake']:\n        directory = f'{base}/{split}/{label}'\n        if not os.path.exists(directory): continue\n        for root, _, files in os.walk(directory):\n            for fname in files:\n                if fname.endswith('.jpg'):\n                    image_paths.append(os.path.join(root, fname))\n                    labels.append(0 if label == 'real' else 1)\n    return image_paths, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:16.033371Z","iopub.execute_input":"2025-07-12T05:24:16.033632Z","iopub.status.idle":"2025-07-12T05:24:16.046408Z","shell.execute_reply.started":"2025-07-12T05:24:16.033607Z","shell.execute_reply":"2025-07-12T05:24:16.045852Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"X_train, y_train = load_images_and_labels('train') # 'processed_faces/train/real/video1/0.jpg'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:16.047036Z","iopub.execute_input":"2025-07-12T05:24:16.047268Z","iopub.status.idle":"2025-07-12T05:24:20.635525Z","shell.execute_reply.started":"2025-07-12T05:24:16.047251Z","shell.execute_reply":"2025-07-12T05:24:20.634766Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"len(X_train),len(y_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:20.636328Z","iopub.execute_input":"2025-07-12T05:24:20.636524Z","iopub.status.idle":"2025-07-12T05:24:20.642148Z","shell.execute_reply.started":"2025-07-12T05:24:20.636507Z","shell.execute_reply":"2025-07-12T05:24:20.641600Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(6710, 6710)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"X_val, y_val = load_images_and_labels('val')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:20.642916Z","iopub.execute_input":"2025-07-12T05:24:20.643164Z","iopub.status.idle":"2025-07-12T05:24:21.644954Z","shell.execute_reply.started":"2025-07-12T05:24:20.643142Z","shell.execute_reply":"2025-07-12T05:24:21.644171Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"print(len(X_val),len(y_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:21.645721Z","iopub.execute_input":"2025-07-12T05:24:21.645949Z","iopub.status.idle":"2025-07-12T05:24:21.650159Z","shell.execute_reply.started":"2025-07-12T05:24:21.645933Z","shell.execute_reply":"2025-07-12T05:24:21.649350Z"}},"outputs":[{"name":"stdout","text":"1500 1500\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"X_test, y_test = load_images_and_labels('Test')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:21.650997Z","iopub.execute_input":"2025-07-12T05:24:21.651223Z","iopub.status.idle":"2025-07-12T05:24:22.841890Z","shell.execute_reply.started":"2025-07-12T05:24:21.651203Z","shell.execute_reply":"2025-07-12T05:24:22.841280Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print(len(X_test),len(y_test))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:22.842973Z","iopub.execute_input":"2025-07-12T05:24:22.843294Z","iopub.status.idle":"2025-07-12T05:24:22.846815Z","shell.execute_reply.started":"2025-07-12T05:24:22.843268Z","shell.execute_reply":"2025-07-12T05:24:22.846092Z"}},"outputs":[{"name":"stdout","text":"1480 1480\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport os\nimport glob\nimport random\n\nclass customDataset(Dataset):\n    def __init__(self, base_dir, split='train', sequence_len=10):\n        self.sequence_len = sequence_len\n        self.samples = []\n        self.split = split\n\n        for label in ['real', 'fake']:\n            folder = os.path.join(base_dir, split, label)\n            if not os.path.exists(folder):\n                continue\n\n            for video_folder in os.listdir(folder):\n                path = os.path.join(folder, video_folder)\n                if os.path.isdir(path):\n                    frames = sorted(glob.glob(os.path.join(path, '*.jpg')))\n                    if len(frames) >= sequence_len:\n                        self.samples.append((frames, 0 if label == 'real' else 1))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        frame_paths, label = self.samples[idx]\n        chosen = sorted(random.sample(frame_paths, self.sequence_len))\n        frames = []\n\n        for fp in chosen:\n            img = Image.open(fp).convert(\"RGB\")\n\n            if label == 0 and self.split == 'train':\n                img = train_real_transforms(img)\n            elif label == 1 and self.split == 'train':\n                img = train_fake_transforms(img)\n            else:\n                img = val_test_transforms(img)\n\n            frames.append(img)\n\n        frames = torch.stack(frames)  # Shape: [T, 3, 224, 224]\n        return frames, torch.tensor(label, dtype=torch.long)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:22.847472Z","iopub.execute_input":"2025-07-12T05:24:22.847722Z","iopub.status.idle":"2025-07-12T05:24:22.861826Z","shell.execute_reply.started":"2025-07-12T05:24:22.847699Z","shell.execute_reply":"2025-07-12T05:24:22.861261Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from torchvision import transforms\n\n# For real images in training\ntrain_real_transforms = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3),\n])\n\n\ntrain_fake_transforms = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomRotation(5),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3),\n])\n\nval_test_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5]*3, [0.5]*3),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:22.862460Z","iopub.execute_input":"2025-07-12T05:24:22.862620Z","iopub.status.idle":"2025-07-12T05:24:22.880989Z","shell.execute_reply.started":"2025-07-12T05:24:22.862607Z","shell.execute_reply":"2025-07-12T05:24:22.880347Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"BATCH_SIZE = 8 # No. of images in a batch ie 128 images in a single batch\nEPOCHS = 10 # Try increasing epochs to 30\nLEARNING_RATE = 3e-4\nPATCH_SIZE = 32 # (P,P)\nNUM_CLASSES = 2\nIMAGE_SIZE = 224 # Transform the image and make the size go to 224\nCHANNELS = 3\nEMBED_DIM = 256\nNUM_HEADS = 4 # INcrease the number heads\nDEPTH = 6 # No. of encoder layers\nMLP_DIM = 512\nDROP_RATE = 0.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:22.881695Z","iopub.execute_input":"2025-07-12T05:24:22.881955Z","iopub.status.idle":"2025-07-12T05:24:22.899031Z","shell.execute_reply.started":"2025-07-12T05:24:22.881932Z","shell.execute_reply":"2025-07-12T05:24:22.898390Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"train_dataset = customDataset(base, split='train', sequence_len=10)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:22.899834Z","iopub.execute_input":"2025-07-12T05:24:22.900516Z","iopub.status.idle":"2025-07-12T05:24:23.271892Z","shell.execute_reply.started":"2025-07-12T05:24:22.900494Z","shell.execute_reply":"2025-07-12T05:24:23.271124Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"print(f\"Loaded {len(train_dataset)} samples in train dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.272720Z","iopub.execute_input":"2025-07-12T05:24:23.273240Z","iopub.status.idle":"2025-07-12T05:24:23.277127Z","shell.execute_reply.started":"2025-07-12T05:24:23.273214Z","shell.execute_reply":"2025-07-12T05:24:23.276384Z"}},"outputs":[{"name":"stdout","text":"Loaded 671 samples in train dataset\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"len(train_dataset) # to detect how many videos are having 10 frames which are used for training ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.277869Z","iopub.execute_input":"2025-07-12T05:24:23.278095Z","iopub.status.idle":"2025-07-12T05:24:23.290534Z","shell.execute_reply.started":"2025-07-12T05:24:23.278072Z","shell.execute_reply":"2025-07-12T05:24:23.289858Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"671"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"val_dataset   = customDataset(base, split='val', sequence_len=10)\nval_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.291276Z","iopub.execute_input":"2025-07-12T05:24:23.291474Z","iopub.status.idle":"2025-07-12T05:24:23.378071Z","shell.execute_reply.started":"2025-07-12T05:24:23.291459Z","shell.execute_reply":"2025-07-12T05:24:23.377284Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print(len(val_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.378660Z","iopub.execute_input":"2025-07-12T05:24:23.378879Z","iopub.status.idle":"2025-07-12T05:24:23.382501Z","shell.execute_reply.started":"2025-07-12T05:24:23.378863Z","shell.execute_reply":"2025-07-12T05:24:23.381907Z"}},"outputs":[{"name":"stdout","text":"150\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"test_dataset  = customDataset(base, split='Test', sequence_len=10)\ntest_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.383169Z","iopub.execute_input":"2025-07-12T05:24:23.383378Z","iopub.status.idle":"2025-07-12T05:24:23.483956Z","shell.execute_reply.started":"2025-07-12T05:24:23.383363Z","shell.execute_reply":"2025-07-12T05:24:23.483218Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"print(len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.484609Z","iopub.execute_input":"2025-07-12T05:24:23.484823Z","iopub.status.idle":"2025-07-12T05:24:23.488515Z","shell.execute_reply.started":"2025-07-12T05:24:23.484807Z","shell.execute_reply":"2025-07-12T05:24:23.487891Z"}},"outputs":[{"name":"stdout","text":"148\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"base","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.492505Z","iopub.execute_input":"2025-07-12T05:24:23.492698Z","iopub.status.idle":"2025-07-12T05:24:23.505634Z","shell.execute_reply.started":"2025-07-12T05:24:23.492682Z","shell.execute_reply":"2025-07-12T05:24:23.505060Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/processed-faces/processed_faces'"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"class TemporalTransformer(nn.Module):\n    def __init__(self, input_dim, num_heads=4, num_layers=2):\n        super().__init__()\n        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, input_dim))\n        self.pos_embedding = nn.Parameter(torch.randn(1, 500, input_dim))\n\n    def forward(self, x):  # x: [B, T, D]\n        B, T, D = x.shape\n        cls_tokens = self.cls_token.expand(B, 1, D)\n        x = torch.cat([cls_tokens, x], dim=1)  # [B, T+1, D]\n        x = x + self.pos_embedding[:, :T+1, :]\n        x = x.permute(1, 0, 2)  # [T+1, B, D]\n        x = self.transformer(x)\n        return x[0]  # CLS token output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.506314Z","iopub.execute_input":"2025-07-12T05:24:23.506515Z","iopub.status.idle":"2025-07-12T05:24:23.520479Z","shell.execute_reply.started":"2025-07-12T05:24:23.506500Z","shell.execute_reply":"2025-07-12T05:24:23.519921Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"len(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.521159Z","iopub.execute_input":"2025-07-12T05:24:23.521341Z","iopub.status.idle":"2025-07-12T05:24:23.537927Z","shell.execute_reply.started":"2025-07-12T05:24:23.521327Z","shell.execute_reply":"2025-07-12T05:24:23.537261Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"671"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# Let's check out what we've created\nprint(f\"DataLoader: {train_loader,val_loader, test_loader}\")\nprint(f\"Length of train_loader: {len(train_loader)} batches of {BATCH_SIZE}...\")\nprint(f\"Length of train_loader: {len(val_loader)} batches of {BATCH_SIZE}...\")\nprint(f\"Length of test_loader: {len(test_loader)} batches of {BATCH_SIZE}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.538645Z","iopub.execute_input":"2025-07-12T05:24:23.538849Z","iopub.status.idle":"2025-07-12T05:24:23.551506Z","shell.execute_reply.started":"2025-07-12T05:24:23.538833Z","shell.execute_reply":"2025-07-12T05:24:23.550896Z"}},"outputs":[{"name":"stdout","text":"DataLoader: (<torch.utils.data.dataloader.DataLoader object at 0x7a4ed8094510>, <torch.utils.data.dataloader.DataLoader object at 0x7a4ed6358150>, <torch.utils.data.dataloader.DataLoader object at 0x7a4ed61dbf90>)\nLength of train_loader: 84 batches of 8...\nLength of train_loader: 19 batches of 8...\nLength of test_loader: 19 batches of 8...\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"21*32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.552231Z","iopub.execute_input":"2025-07-12T05:24:23.552438Z","iopub.status.idle":"2025-07-12T05:24:23.566947Z","shell.execute_reply.started":"2025-07-12T05:24:23.552424Z","shell.execute_reply":"2025-07-12T05:24:23.566221Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"672"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"5*32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.567804Z","iopub.execute_input":"2025-07-12T05:24:23.567965Z","iopub.status.idle":"2025-07-12T05:24:23.579396Z","shell.execute_reply.started":"2025-07-12T05:24:23.567949Z","shell.execute_reply":"2025-07-12T05:24:23.578654Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"160"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self,\n                 img_size,\n                 patch_size,\n                 in_channels,\n                 embed_dim):\n        super().__init__()\n        self.patch_size = patch_size\n        self.proj = nn.Conv2d(in_channels=in_channels,\n                              out_channels=embed_dim,\n                              kernel_size=patch_size,\n                              stride=patch_size)\n        num_patches = (img_size // patch_size) ** 2\n        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim)) # [Batch singleton, 1 CLS token for every image, Embedding Dimension]\n        self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim)) # [Batch singleton, Token positions (1 for [CLS] token + num_patches for image patches), Embedding dimension]\n\n    def forward(self, x: torch.Tensor):\n        B = x.size(0) # x -> (B, C, H, W)\n        x = self.proj(x) # (B, E, H/P, W/P) ie for every image in B( batch size) divided into no. of patches (H/P,W/P) and each patch has output dime after projection as Embedding Dimension\n        x = x.flatten(2).transpose(1, 2) # flatten(H/P, W/P) -> N (No of patches) -> (B, E, N) -> transpose -> (B, N, E)\n        cls_tokens = self.cls_token.expand(B, -1, -1) # (1, 1, E) -> (B, 1, E) Expands the learned [CLS] token to batch size B\n        # x: (B,C,H,W) -> (B,E,H/P,W/P) -> (B,N,E), cls_token: (1,1,E)->(B,1,E)\n        x = torch.cat((cls_tokens, x), dim=1) # x: (B,N,E) & cls_token: (B,1,E) -> (B,1+N,E)embed a [CLS] token with every image(containing N patches)\n        x = x + self.pos_embed # (B,1+N,E) -> (B,1+N,E) actually pos_embed is broadcasted to B batches from 1 batch\n        return x # (B,1+N,E)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.580163Z","iopub.execute_input":"2025-07-12T05:24:23.580362Z","iopub.status.idle":"2025-07-12T05:24:23.593103Z","shell.execute_reply.started":"2025-07-12T05:24:23.580343Z","shell.execute_reply":"2025-07-12T05:24:23.592391Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self,\n                 in_features,\n                 hidden_features,\n                 drop_rate):\n        super().__init__()\n        \n        self.network = nn.Sequential(\n            nn.Linear(in_features, hidden_features),\n            nn.GELU(),\n            nn.Dropout(p=drop_rate),\n            nn.Linear(hidden_features, in_features),\n            nn.Dropout(p=drop_rate)\n        )\n\n    def forward(self, x):\n        x = self.network(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.593695Z","iopub.execute_input":"2025-07-12T05:24:23.593912Z","iopub.status.idle":"2025-07-12T05:24:23.609749Z","shell.execute_reply.started":"2025-07-12T05:24:23.593889Z","shell.execute_reply":"2025-07-12T05:24:23.609179Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class TransformerEncoderLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_dim, drop_rate):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop_rate, batch_first=True)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mlp = MLP(embed_dim, mlp_dim, drop_rate)\n\n    def forward(self, x):\n        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n        x = x + self.mlp(self.norm2(x))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.610394Z","iopub.execute_input":"2025-07-12T05:24:23.610575Z","iopub.status.idle":"2025-07-12T05:24:23.627902Z","shell.execute_reply.started":"2025-07-12T05:24:23.610561Z","shell.execute_reply":"2025-07-12T05:24:23.627124Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# class DeepFakeDetector(nn.Module):\n#     def __init__(self,\n#                  img_size,\n#                  patch_size,\n#                  in_channels,\n#                  num_classes,\n#                  embed_dim,\n#                  depth,\n#                  num_heads,\n#                  mlp_dim,\n#                  drop_rate,\n#                  temporal_layers=2,\n#                  temporal_heads=4):\n#         super().__init__()\n\n#         # Vision Transformer backbone (no final head)\n#         self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n#         self.encoder = nn.Sequential(*[\n#             TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, drop_rate)\n#             for _ in range(depth)\n#         ])\n#         self.norm = nn.LayerNorm(embed_dim)\n\n#         # Temporal transformer for sequence modeling\n#         self.temporal_transformer = TemporalTransformer(\n#             input_dim=embed_dim,\n#             num_heads=temporal_heads,\n#             num_layers=temporal_layers\n#         )\n\n#         # Final classification head (after temporal aggregation)\n#         self.head = nn.Linear(embed_dim, num_classes)\n\n#     def forward(self, x):\n#         \"\"\"\n#         x: Tensor of shape [B, T, C, H, W]\n#         \"\"\"\n#         B, T, C, H, W = x.shape\n    \n#         # Reshape to feed each frame through ViT individually\n#         x = x.view(B * T, C, H, W)  # [B*T, C, H, W]\n    \n#         # Pass through patch embedding and transformer encoder\n#         x = self.patch_embed(x)     # [B*T, 1+N, D]\n#         x = self.encoder(x)         # [B*T, 1+N, D]\n#         x = self.norm(x)            # [B*T, 1+N, D]\n#         cls_tokens = x[:, 0]        # Extract CLS token: [B*T, D]\n    \n#         # Reshape back to video format\n#         cls_tokens = cls_tokens.view(B, T, -1)  # [B, T, D]\n    \n#         # Temporal modeling\n#         video_repr = self.temporal_transformer(cls_tokens)  # [B, D]\n    \n#         # Classification\n#         out = self.head(video_repr)  # [B, num_classes]\n#         return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.628708Z","iopub.execute_input":"2025-07-12T05:24:23.628973Z","iopub.status.idle":"2025-07-12T05:24:23.641296Z","shell.execute_reply.started":"2025-07-12T05:24:23.628951Z","shell.execute_reply":"2025-07-12T05:24:23.640767Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\n\nclass DeepFakeDetector(nn.Module):\n    def __init__(self, pretrained_model='vit_base_patch16_224', num_classes=2, temporal_layers=2, temporal_heads=4):\n        super().__init__()\n        \n        # Load pretrained ViT from timm (no head)\n        self.vit = timm.create_model(pretrained_model, pretrained=True)\n        self.vit.head = nn.Identity()  # Remove classification head\n\n        # Freeze ViT \n        for param in self.vit.parameters():\n            param.requires_grad = False\n\n        self.temporal = TemporalTransformer(\n            input_dim=self.vit.embed_dim,\n            num_heads=temporal_heads,\n            num_layers=temporal_layers\n        )\n\n        self.head = nn.Linear(self.vit.embed_dim, num_classes)\n\n    def forward(self, x):\n        \"\"\"\n        x shape: [B, T, C, H, W] where T = number of frames\n        \"\"\"\n        B, T, C, H, W = x.shape\n        x = x.view(B * T, C, H, W)               # [B*T, C, H, W]\n        features = self.vit(x)                   # [B*T, D]\n        features = features.view(B, T, -1)       # [B, T, D]\n\n        temporal_out = self.temporal(features)   # [B, D]\n        return self.head(temporal_out)           # [B, num_classes]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.642048Z","iopub.execute_input":"2025-07-12T05:24:23.642498Z","iopub.status.idle":"2025-07-12T05:24:23.659351Z","shell.execute_reply.started":"2025-07-12T05:24:23.642473Z","shell.execute_reply":"2025-07-12T05:24:23.658841Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"model = DeepFakeDetector().to(DEVICE)\n# model = nn.DataParallel(model)\n# model = model.to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:23.660031Z","iopub.execute_input":"2025-07-12T05:24:23.660260Z","iopub.status.idle":"2025-07-12T05:24:26.980011Z","shell.execute_reply.started":"2025-07-12T05:24:23.660240Z","shell.execute_reply":"2025-07-12T05:24:26.979145Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"426ee13faee74482b901abf5788fa946"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"optimizer = torch.optim.Adam([\n    {'params': model.parameters(), 'lr': LEARNING_RATE}\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:26.980877Z","iopub.execute_input":"2025-07-12T05:24:26.981162Z","iopub.status.idle":"2025-07-12T05:24:26.986365Z","shell.execute_reply.started":"2025-07-12T05:24:26.981139Z","shell.execute_reply":"2025-07-12T05:24:26.985590Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"loss_fn = nn.CrossEntropyLoss()\nepochs = EPOCHS","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:26.987163Z","iopub.execute_input":"2025-07-12T05:24:26.987547Z","iopub.status.idle":"2025-07-12T05:24:27.002063Z","shell.execute_reply.started":"2025-07-12T05:24:26.987523Z","shell.execute_reply":"2025-07-12T05:24:27.001485Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"def train(model, loader, optimizer, criterion):\n    # Set the mode of the model into training\n    model.train()\n    device = DEVICE\n    total_loss, correct = 0, 0\n\n    for x, y in loader:\n        # Moving (Sending) our data into the target device\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        # 1. Forward pass (model outputs raw logits)\n        out = model(x)\n        # 2. Calcualte loss (per batch)\n        loss = criterion(out, y)\n        # 3. Perform backpropgation\n        loss.backward()\n        # 4. Perforam Gradient Descent\n        optimizer.step()\n\n        total_loss += loss.item() * x.size(0)\n        correct += (out.argmax(1) == y).sum().item()\n    # You have to scale the loss (Normlization step to make the loss general across all batches)\n    return total_loss / len(loader.dataset), correct / len(loader.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:27.002725Z","iopub.execute_input":"2025-07-12T05:24:27.002963Z","iopub.status.idle":"2025-07-12T05:24:27.019146Z","shell.execute_reply.started":"2025-07-12T05:24:27.002938Z","shell.execute_reply":"2025-07-12T05:24:27.018370Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def evaluate(model, loader):\n    model.eval() # Set the mode of the model into evlauation\n    correct = 0\n    device = DEVICE\n    with torch.inference_mode():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            correct += (out.argmax(dim=1) == y).sum().item()\n    return correct / len(loader.dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:27.019899Z","iopub.execute_input":"2025-07-12T05:24:27.020125Z","iopub.status.idle":"2025-07-12T05:24:27.037245Z","shell.execute_reply.started":"2025-07-12T05:24:27.020109Z","shell.execute_reply":"2025-07-12T05:24:27.036595Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"from tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:27.037942Z","iopub.execute_input":"2025-07-12T05:24:27.038155Z","iopub.status.idle":"2025-07-12T05:24:27.052168Z","shell.execute_reply.started":"2025-07-12T05:24:27.038135Z","shell.execute_reply":"2025-07-12T05:24:27.051489Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"### Training\ntrain_accuracies, test_accuracies = [], []\n\nfor epoch in tqdm(range(EPOCHS)):\n    train_loss, train_acc = train(model, train_loader, optimizer, loss_fn)\n    test_acc = evaluate(model, test_loader)\n    train_accuracies.append(train_acc)\n    test_accuracies.append(test_acc)\n    print(f\"Epoch: {epoch+1}/{EPOCHS}, Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}%, Test acc: {test_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:24:27.052921Z","iopub.execute_input":"2025-07-12T05:24:27.053158Z","iopub.status.idle":"2025-07-12T05:33:05.647948Z","shell.execute_reply.started":"2025-07-12T05:24:27.053136Z","shell.execute_reply":"2025-07-12T05:33:05.646936Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"462e391f1b9943d28b8f0283b63de87d"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1/10, Train loss: 0.8850, Train acc: 0.5127%, Test acc: 0.4932\nEpoch: 2/10, Train loss: 0.7359, Train acc: 0.5663%, Test acc: 0.5203\nEpoch: 3/10, Train loss: 0.6193, Train acc: 0.6453%, Test acc: 0.5338\nEpoch: 4/10, Train loss: 0.5767, Train acc: 0.7034%, Test acc: 0.5405\nEpoch: 5/10, Train loss: 0.6959, Train acc: 0.6080%, Test acc: 0.5068\nEpoch: 6/10, Train loss: 0.7154, Train acc: 0.4858%, Test acc: 0.5068\nEpoch: 7/10, Train loss: 0.6793, Train acc: 0.5350%, Test acc: 0.5000\nEpoch: 8/10, Train loss: 0.7345, Train acc: 0.5410%, Test acc: 0.4932\nEpoch: 9/10, Train loss: 0.6608, Train acc: 0.5872%, Test acc: 0.4932\nEpoch: 10/10, Train loss: 0.7083, Train acc: 0.5306%, Test acc: 0.4932\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"### Ignore","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model,train_loader,optimizer,loss_fn):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n    for batch_features, batch_labels in train_loader:\n            batch_features = batch_features.to(DEVICE)\n            batch_labels = batch_labels.to(DEVICE)\n            optimizer.zero_grad()\n            y_pred = model(batch_features)               # [B, 2]\n            loss = loss_fn(y_pred, batch_labels)         # labels: [B]\n            _, predicted = torch.max(y_pred, 1) \n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()         # [B]\n            correct += (predicted == batch_labels).sum().item()\n            total += batch_labels.size(0)\n    \n    avg_loss = total_loss / len(train_loader)\n    accuracy = correct / total\n    return avg_loss,accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:33:05.649087Z","iopub.execute_input":"2025-07-12T05:33:05.649379Z","iopub.status.idle":"2025-07-12T05:33:05.655701Z","shell.execute_reply.started":"2025-07-12T05:33:05.649342Z","shell.execute_reply":"2025-07-12T05:33:05.654997Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def evaluate_loss_and_accuracy(model, loader, loss_fn):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(DEVICE), y.to(DEVICE)\n            y_pred = model(x)\n            loss = loss_fn(y_pred, y)\n            _,predicted = torch.max(y_pred,dim=1)\n            total_loss += loss.item() \n            correct += (predicted == y).sum().item()\n            total += y.size(0)\n\n    avg_loss = total_loss / len(loader)\n    accuracy = correct / total\n    return avg_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:33:05.656458Z","iopub.execute_input":"2025-07-12T05:33:05.656644Z","iopub.status.idle":"2025-07-12T05:33:05.674268Z","shell.execute_reply.started":"2025-07-12T05:33:05.656628Z","shell.execute_reply":"2025-07-12T05:33:05.673533Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"from sklearn.metrics import balanced_accuracy_score\nimport torch\nimport os\n\ndef train_model(model, train_loader, val_loader, test_loader=None, optimizer=None, loss_fn=None,\n                epochs=20, model_save_path='best_model.pth'):\n    \n    train_losses, val_losses, val_accuracies = [], [], []\n    best_val_acc = 0\n    best_model_state = None\n\n    for epoch in range(epochs):\n        # Train for one epoch\n        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, loss_fn)\n        \n        # Validation loss and accuracy\n        val_loss, val_acc = evaluate_loss_and_accuracy(model, val_loader, loss_fn)\n\n        # Logging\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        val_accuracies.append(val_acc)\n\n        print(f\"Epoch {epoch+1}: \"\n              f\"Train Loss = {train_loss:.4f}, \"\n              f\"Val Loss = {val_loss:.4f}, \"\n              f\"Val Acc = {val_acc:.4f}\")\n\n        # Save best model based on validation accuracy\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            best_model_state = model.state_dict()\n            torch.save(best_model_state, model_save_path)\n            print(f\" New best model saved with Val Acc = {val_acc:.4f}\")\n\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n\n    # Load best saved model after all epochs\n    if os.path.exists(model_save_path):\n        model.load_state_dict(torch.load(model_save_path))\n\n    # Plot learning curves\n    plot_learning_curves(train_losses, val_losses, val_accuracies)\n\n    # # Tune threshold using validation set\n    # val_probs, val_targets = get_predictions_and_targets(model, val_loader)\n    # best_thresh = find_best_threshold(val_targets, val_probs)\n    # return best_thresh\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:33:05.674974Z","iopub.execute_input":"2025-07-12T05:33:05.675482Z","iopub.status.idle":"2025-07-12T05:33:05.698482Z","shell.execute_reply.started":"2025-07-12T05:33:05.675455Z","shell.execute_reply":"2025-07-12T05:33:05.697702Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_learning_curves(train_losses, val_losses, val_accuracies):\n    epochs_range = range(1, len(train_losses) + 1)\n    plt.figure(figsize=(14, 5))\n\n    # Loss Plot\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, train_losses, label='Train Loss')\n    plt.plot(epochs_range, val_losses, label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss Curve')\n    plt.grid(True)\n    plt.legend()\n\n    # Accuracy Plot\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, val_accuracies, label='Val Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Validation Accuracy Curve')\n    plt.grid(True)\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:33:05.699252Z","iopub.execute_input":"2025-07-12T05:33:05.699439Z","iopub.status.idle":"2025-07-12T05:33:05.718359Z","shell.execute_reply.started":"2025-07-12T05:33:05.699425Z","shell.execute_reply":"2025-07-12T05:33:05.717771Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"train_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    optimizer=optimizer,\n    loss_fn=loss_fn,\n    epochs=20\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T05:33:05.719114Z","iopub.execute_input":"2025-07-12T05:33:05.719290Z"}},"outputs":[{"name":"stdout","text":"Epoch 1: Train Loss = 0.6662, Val Loss = 0.7135, Val Acc = 0.5600\n New best model saved with Val Acc = 0.5600\nEpoch 2: Train Loss = 0.6891, Val Loss = 0.6215, Val Acc = 0.6000\n New best model saved with Val Acc = 0.6000\nEpoch 3: Train Loss = 0.6328, Val Loss = 0.6353, Val Acc = 0.6267\n New best model saved with Val Acc = 0.6267\nEpoch 4: Train Loss = 0.5802, Val Loss = 0.7164, Val Acc = 0.5600\nEpoch 5: Train Loss = 0.5626, Val Loss = 1.3041, Val Acc = 0.5067\nEpoch 6: Train Loss = 0.5481, Val Loss = 1.1752, Val Acc = 0.5200\nEpoch 7: Train Loss = 0.5908, Val Loss = 0.9071, Val Acc = 0.5133\nEpoch 8: Train Loss = 0.5457, Val Loss = 1.3478, Val Acc = 0.5200\nEpoch 9: Train Loss = 0.5513, Val Loss = 0.8856, Val Acc = 0.5067\nEpoch 10: Train Loss = 0.5215, Val Loss = 1.2916, Val Acc = 0.5133\nEpoch 11: Train Loss = 0.4997, Val Loss = 1.4342, Val Acc = 0.5400\nEpoch 12: Train Loss = 0.5815, Val Loss = 0.8749, Val Acc = 0.5200\nEpoch 13: Train Loss = 0.6139, Val Loss = 0.6886, Val Acc = 0.5333\nEpoch 14: Train Loss = 0.5647, Val Loss = 1.1014, Val Acc = 0.5133\nEpoch 15: Train Loss = 0.4600, Val Loss = 1.3834, Val Acc = 0.5067\nEpoch 16: Train Loss = 0.4914, Val Loss = 1.4921, Val Acc = 0.5200\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### Test Dataset","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    correct = 0\n    total = 0\n    device = DEVICE\n    y_true, y_pred, y_probs = [], [], []\n    for batch_features,batch_labels in test_loader:\n        batch_features = batch_features.to(device)\n        batch_labels = batch_labels.to(device)\n        prob = model(batch_features)\n        _,pred = torch.max(prob,dim=1)\n        correct += (pred == batch_labels).sum().item()\n        total += batch_labels.size(0)\n        y_true.append(batch_labels.cpu().numpy())\n        y_probs.append(prob.cpu().numpy())\n        y_pred.append(pred.cpu().numpy())\n        # print(total)\n    print(f'Test Accuracy: {correct/total:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Ensure proper format\ny_true = np.array(y_true).astype(int).ravel()\ny_pred = np.array(y_pred).astype(int).ravel()\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n\n# Plot\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n            xticklabels=[\"Real\", \"Fake\"],\n            yticklabels=[\"Real\", \"Fake\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix on Test Set\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import (\n    f1_score,\n    precision_score,\n    recall_score,\n    balanced_accuracy_score,\n    classification_report,\n    confusion_matrix,\n    matthews_corrcoef\n)\nbalanced_acc = balanced_accuracy_score(y_true, y_pred)\nprint(f\"Balanced Accuracy: {balanced_acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(y_true, y_pred)\nTN, FP, FN, TP = cm.ravel()\n\nsensitivity = TP / (TP + FN)\nspecificity = TN / (TN + FP)\n\ngmean = np.sqrt(sensitivity * specificity)\nprint(f\"G-mean: {gmean:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mcc = matthews_corrcoef(y_true, y_pred)\nprint(f\"MCC: {mcc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}